{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from cvxopt import matrix\n",
    "from cvxopt.blas import dot\n",
    "from cvxopt.solvers import qp\n",
    "import pickle\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "import statsmodels.api as sm\n",
    "from sklearn import decomposition\n",
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from numpy import linalg\n",
    "import statsmodels\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_object(file_name):\n",
    "    \"load the pickled object\"\n",
    "    with open(file_name, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "\n",
    "def view_data(data_path):\n",
    "    data = load_object(data_path)\n",
    "    prices = data['prices']\n",
    "    names = data['features']['names']\n",
    "    features = data['features']['values']\n",
    "    print(prices.shape)\n",
    "    print(names)\n",
    "    print(features.shape)\n",
    "    return prices, features, names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(757, 680)\n",
      "['labour cost', 'analyst projected total earnings', 'weighted average outstanding shares', 'R&D intensity index', 'relative strength index', 'total assets', 'net book value', 'analyst sentiment', 'market share', 'Aggregate Capital Cost']\n",
      "(756, 680, 10)\n"
     ]
    }
   ],
   "source": [
    "prices, features, names = view_data('C3_train.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_assets = len(prices[0])\n",
    "\n",
    "n_obs = m = len(prices[:756,])\n",
    "\n",
    "print(n_assets, n_obs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a returns vector using prices. Loses one DF\n",
    "df = pd.DataFrame(prices)\n",
    "returns_df = df.pct_change()\n",
    "returns_df.drop(0, axis = 0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(returns_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(returns_df.T, alpha=.4);\n",
    "plt.xlabel('time')\n",
    "plt.ylabel('returns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_weights(n):\n",
    "    ''' Produces n random weights that sum to 1 '''\n",
    "    k = np.random.rand(n)\n",
    "    return k / sum(k)\n",
    "\n",
    "# print(rand_weights(n_assets))\n",
    "# print(rand_weights(n_assets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_portfolio(returns):\n",
    "    ''' \n",
    "    Returns the mean and standard deviation of returns for a random portfolio\n",
    "    '''\n",
    "\n",
    "    p = np.asmatrix(np.mean(returns, axis=1))\n",
    "    w = np.asmatrix(rand_weights(returns.shape[0]))\n",
    "    C = np.asmatrix(np.cov(returns))\n",
    "    \n",
    "    mu = w * p.T\n",
    "    sigma = np.sqrt(w * C * w.T)\n",
    "    \n",
    "    # This recursion reduces outliers to keep plots pretty\n",
    "    if sigma > 2:\n",
    "        return random_portfolio(returns)\n",
    "    return mu, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_portfolios = 500\n",
    "means, stds = np.column_stack([\n",
    "    random_portfolio(returns_df) \n",
    "    for _ in range(n_portfolios)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(stds, means, 'o', markersize=5)\n",
    "plt.xlabel('std')\n",
    "plt.ylabel('mean')\n",
    "plt.title('Mean and standard deviation of returns of randomly generated portfolios')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_portfolio(returns):\n",
    "    n = len(returns)\n",
    "    returns = np.asmatrix(returns)\n",
    "    \n",
    "    N = 100\n",
    "    mus = [10**(5.0 * t/N - 1.0) for t in range(N)]\n",
    "    \n",
    "    # Convert to cvxopt matrices\n",
    "    S = opt.matrix(np.cov(returns))\n",
    "    pbar = opt.matrix(np.mean(returns, axis=1))\n",
    "    \n",
    "    # Create constraint matrices\n",
    "    G = -opt.matrix(np.eye(n))   # negative n x n identity matrix\n",
    "    h = opt.matrix(0.0, (n ,1))\n",
    "    A = opt.matrix(1.0, (1, n))\n",
    "    b = opt.matrix(1.0)\n",
    "    \n",
    "    # Calculate efficient frontier weights using quadratic programming\n",
    "    portfolios = [solvers.qp(mu*S, -pbar, G, h, A, b)['x'] \n",
    "                  for mu in mus]\n",
    "    ## CALCULATE RISKS AND RETURNS FOR FRONTIER\n",
    "    returns = [blas.dot(pbar, x) for x in portfolios]\n",
    "    risks = [np.sqrt(blas.dot(x, S*x)) for x in portfolios]\n",
    "    ## CALCULATE THE 2ND DEGREE POLYNOMIAL OF THE FRONTIER CURVE\n",
    "    m1 = np.polyfit(returns, risks, 2)\n",
    "    x1 = np.sqrt(m1[2] / m1[0])\n",
    "    # CALCULATE THE OPTIMAL PORTFOLIO\n",
    "    wt = solvers.qp(opt.matrix(x1 * S), -pbar, G, h, A, b)['x']\n",
    "    return np.asarray(wt), returns, risks\n",
    "\n",
    "weights, returns, risks = optimal_portfolio(returns_df.T)\n",
    "\n",
    "plt.plot(stds, means, 'o')\n",
    "plt.ylabel('mean')\n",
    "plt.xlabel('std')\n",
    "plt.plot(risks, returns, 'y-o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(returns).mean() * math.sqrt(254) / np.array(risks).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#S0 to F0\n",
    "\n",
    "s0 = returns_df.iloc[-50:-1,0] \n",
    "f0 = features[-50:-1,0]\n",
    "f1squared = np.square(f0)\n",
    "f0 = np.concatenate([f0,f1squared], axis = 1)\n",
    "\n",
    "\n",
    "#Pull data\n",
    "X_train, X_test, y_train, y_test = train_test_split(f1, s1, test_size=0.2, random_state=0) \n",
    "\n",
    "#Standardize\n",
    "sc = StandardScaler()  \n",
    "X_train = sc.fit_transform(X_train)  \n",
    "X_test = sc.transform(X_test)  \n",
    "\n",
    "#PCA on 6 components\n",
    "pca = decomposition.PCA(n_components = 3)  \n",
    "X_train = pca.fit_transform(X_train)\n",
    "X_trainsquare = np.square(X_train)\n",
    "X_train = np.concatenate([X_train,X_trainsquare], axis = 1)\n",
    "X_test = pca.transform(X_test)\n",
    "X_testsquare = np.square(X_test)\n",
    "X_test = np.concatenate([X_test,X_testsquare], axis = 1)\n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "explained_variance = pca.explained_variance_ratio_  \n",
    "print(explained_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg1 = model.fit(X_train, y_train)\n",
    "(reg1.coef_)\n",
    "print(reg1.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F0 to S1\n",
    "\n",
    "s1 = prices[-50:-1,0] \n",
    "f0 = features[-51:-2,0]\n",
    "f1squared = np.square(f0)\n",
    "f0 = np.concatenate([f0,f1squared], axis = 1)\n",
    "\n",
    "\n",
    "#Pull data\n",
    "X_train, X_test, y_train, y_test = train_test_split(f0, s1, test_size=0.2, random_state=0) \n",
    "\n",
    "#Standardize\n",
    "sc = StandardScaler()  \n",
    "X_train = sc.fit_transform(X_train)  \n",
    "X_test = sc.transform(X_test)  \n",
    "\n",
    "#PCA on 6 components\n",
    "pca = decomposition.PCA(n_components = 3)  \n",
    "X_train = pca.fit_transform(X_train)\n",
    "X_trainsquare = np.square(X_train)\n",
    "X_train = np.concatenate([X_train,X_trainsquare], axis = 1)\n",
    "X_test = pca.transform(X_test)\n",
    "X_testsquare = np.square(X_test)\n",
    "X_test = np.concatenate([X_test,X_testsquare], axis = 1)\n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "explained_variance = pca.explained_variance_ratio_  \n",
    "print(explained_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg1 = model.fit(X_train, y_train)\n",
    "print(reg1.coef_)\n",
    "print(reg1.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#S0 to F0\n",
    "\n",
    "s0 = returns_df.iloc[-50:-1,0] \n",
    "f0 = features[-50:-1,0]\n",
    "#f1squared = np.square(f0)\n",
    "#f0 = np.concatenate([f0,f1squared], axis = 1)\n",
    "\n",
    "\n",
    "#Pull data\n",
    "#X_train, X_test, y_train, y_test = train_test_split(f1, s1, test_size=0.2, random_state=0) \n",
    "\n",
    "#Standardize\n",
    "sc = StandardScaler()  \n",
    "X_train = sc.fit_transform(f0)  \n",
    "#X_test = sc.transform(X_test)  \n",
    "\n",
    "#PCA on 6 components\n",
    "pca = decomposition.PCA(n_components = 5)  \n",
    "X_train = pca.fit_transform(X_train)\n",
    "#X_trainsquare = np.square(X_train)\n",
    "#X_train = np.concatenate([X_train,X_trainsquare], axis = 1)\n",
    "#X_test = pca.transform(X_test)\n",
    "#X_testsquare = np.square(X_test)\n",
    "#X_test = np.concatenate([X_test,X_testsquare], axis = 1)\n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "explained_variance = pca.explained_variance_ratio_  \n",
    "print(explained_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reg1 = model.fit(X_train, s0)\n",
    "print(reg1.coef_)\n",
    "print(reg1.score(X_train, s0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Say we are given some new price/factors vector\n",
    "org_prices, org_features, names = view_data('C3_train.pkl')\n",
    "\n",
    "#Just take the first 500 features and the 501 prices, Just like train data\n",
    "prices = org_prices[0:501]\n",
    "features = org_features[0:500]\n",
    "\n",
    "\n",
    "\n",
    "#Log the price vector\n",
    "df = pd.DataFrame(prices)\n",
    "returns_df = df.pct_change()\n",
    "returns_df.drop(0, axis = 0, inplace = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#S0 to F0 for stock 0\n",
    "pca = decomposition.PCA(n_components = 3)\n",
    "model = LinearRegression()\n",
    "sc = StandardScaler()\n",
    "\n",
    "s0 = returns_df.iloc[-51:-2,0] \n",
    "f0 = features[:,0].copy()\n",
    "df = pd.DataFrame(f0)\n",
    "featureschange_df = df.pct_change()\n",
    "featureschange_df.drop(0, axis = 0, inplace = True)\n",
    "f0 = featureschange_df.T.iloc[:,-51:-2]\n",
    "\n",
    "\n",
    "\n",
    "#Standardize\n",
    " \n",
    "#f0 = sc.fit_transform(f0)  \n",
    "\n",
    "\n",
    "#PCA on 5 components\n",
    "f0 = pca.fit_transform(f0.T)\n",
    "\n",
    "\n",
    "#Fit\n",
    "reg1 = model.fit(f0, s0)\n",
    "coef = reg1.coef_\n",
    "\n",
    "s1 = (prices[-1][0]  - prices[-2][0]) / prices[-2][0] \n",
    "predicted_f1 = coef.T * s1\n",
    "\n",
    "\n",
    "######################################################################\n",
    "\n",
    "\n",
    "# F1 to S2\n",
    "\n",
    "s1 = returns_df.iloc[-50:-1,0] \n",
    "f0 = features[:,0].copy()\n",
    "df = pd.DataFrame(f0)\n",
    "featureschange_df = df.pct_change()\n",
    "featureschange_df.drop(0, axis = 0, inplace = True)\n",
    "f0 = featureschange_df.T.iloc[:,-51:-2]\n",
    "\n",
    "f0 = pca.fit_transform(f0.T)\n",
    "#f0_sqr = np.square(f0)\n",
    "#f0 = np.concatenate([f0, f0_sqr], axis = 1)\n",
    "\n",
    "\n",
    "reg2 = model.fit(f0, s1)\n",
    "predicted_f1\n",
    "#predicted_f1squared = np.square(predicted_f1)\n",
    "#predicted_f1 = np.concatenate([predicted_f1, predicted_f1squared])\n",
    "\n",
    "x = reg2.predict(predicted_f1.reshape(1,-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#S0 to F0 for stock 1\n",
    "pca = decomposition.PCA(n_components = 3)\n",
    "model = LinearRegression()\n",
    "sc = StandardScaler()\n",
    "\n",
    "s0 = returns_df.iloc[-51:-2,1] \n",
    "f0 = features[:,1].copy()\n",
    "df = pd.DataFrame(f0)\n",
    "featureschange_df = df.pct_change()\n",
    "featureschange_df.drop(0, axis = 0, inplace = True)\n",
    "f0 = featureschange_df.T.iloc[:,-51:-2]\n",
    "\n",
    "\n",
    "\n",
    "#Standardize\n",
    " \n",
    "#f0 = sc.fit_transform(f0)  \n",
    "\n",
    "\n",
    "#PCA on 5 components\n",
    "f0 = pca.fit_transform(f0.T)\n",
    "\n",
    "\n",
    "#Fit\n",
    "reg1 = model.fit(f0, s0)\n",
    "coef = reg1.coef_\n",
    "\n",
    "s1 = (prices[-1][1]  - prices[-2][1]) / prices[-2][1] \n",
    "predicted_f1 = coef.T * s1\n",
    "\n",
    "\n",
    "######################################################################\n",
    "\n",
    "\n",
    "# F1 to S2\n",
    "\n",
    "s1 = returns_df.iloc[-50:-1,1] \n",
    "f0 = features[:,1].copy()\n",
    "df = pd.DataFrame(f0)\n",
    "featureschange_df = df.pct_change()\n",
    "featureschange_df.drop(0, axis = 0, inplace = True)\n",
    "f0 = featureschange_df.T.iloc[:,-51:-2]\n",
    "\n",
    "f0 = pca.fit_transform(f0.T)\n",
    "#f0_sqr = np.square(f0)\n",
    "#f0 = np.concatenate([f0, f0_sqr], axis = 1)\n",
    "\n",
    "\n",
    "reg2 = model.fit(f0, s1)\n",
    "predicted_f1\n",
    "#predicted_f1squared = np.square(predicted_f1)\n",
    "#predicted_f1 = np.concatenate([predicted_f1, predicted_f1squared])\n",
    "\n",
    "x = reg2.predict(predicted_f1.reshape(1,-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#S0 to F0 for stock 2\n",
    "pca = decomposition.PCA(n_components = 3)\n",
    "model = LinearRegression()\n",
    "sc = StandardScaler()\n",
    "\n",
    "s0 = returns_df.iloc[-51:-2,2] \n",
    "f0 = features[:,2].copy()\n",
    "df = pd.DataFrame(f0)\n",
    "featureschange_df = df.pct_change()\n",
    "featureschange_df.drop(0, axis = 0, inplace = True)\n",
    "f0 = featureschange_df.T.iloc[:,-51:-2]\n",
    "\n",
    "\n",
    "\n",
    "#Standardize\n",
    " \n",
    "#f0 = sc.fit_transform(f0)  \n",
    "\n",
    "\n",
    "#PCA on 5 components\n",
    "f0 = pca.fit_transform(f0.T)\n",
    "\n",
    "\n",
    "#Fit\n",
    "reg1 = model.fit(f0, s0)\n",
    "coef = reg1.coef_\n",
    "\n",
    "s1 = (prices[-1][2]  - prices[-2][2]) / prices[-2][2] \n",
    "predicted_f1 = coef.T * s1\n",
    "\n",
    "\n",
    "######################################################################\n",
    "\n",
    "\n",
    "# F1 to S2\n",
    "\n",
    "s1 = returns_df.iloc[-50:-1,2] \n",
    "f0 = features[:,2].copy()\n",
    "df = pd.DataFrame(f0)\n",
    "featureschange_df = df.pct_change()\n",
    "featureschange_df.drop(0, axis = 0, inplace = True)\n",
    "f0 = featureschange_df.T.iloc[:,-51:-2]\n",
    "\n",
    "f0 = pca.fit_transform(f0.T)\n",
    "#f0_sqr = np.square(f0)\n",
    "#f0 = np.concatenate([f0, f0_sqr], axis = 1)\n",
    "\n",
    "\n",
    "reg2 = model.fit(f0, s1)\n",
    "predicted_f1\n",
    "#predicted_f1squared = np.square(predicted_f1)\n",
    "#predicted_f1 = np.concatenate([predicted_f1, predicted_f1squared])\n",
    "\n",
    "x = reg2.predict(predicted_f1.reshape(1,-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arbitrary Stock Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-2b2a1d726bab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m#PCA on 5 components\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mf0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/decomposition/pca.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \"\"\"\n\u001b[0;32m--> 359\u001b[0;31m         \u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m         \u001b[0mU\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mU\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_components_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/decomposition/pca.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m         X = check_array(X, dtype=[np.float64, np.float32], ensure_2d=True,\n\u001b[0;32m--> 381\u001b[0;31m                         copy=self.copy)\n\u001b[0m\u001b[1;32m    382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;31m# Handle n_components==None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m             _assert_all_finite(array,\n\u001b[0;32m--> 573\u001b[0;31m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0m\u001b[1;32m    574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m     \u001b[0mshape_repr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_shape_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan)\u001b[0m\n\u001b[1;32m     54\u001b[0m                 not allow_nan and not np.isfinite(X).all()):\n\u001b[1;32m     55\u001b[0m             \u001b[0mtype_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'infinity'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mallow_nan\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'NaN, infinity'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg_err\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "\n",
    "q = []\n",
    "for k in range(0,len(prices[-1])):    \n",
    "    #S0 to F0\n",
    "    pca = decomposition.PCA(n_components = 3)\n",
    "    model = LinearRegression()\n",
    "    sc = StandardScaler()\n",
    "\n",
    "    s0 = returns_df.iloc[-51:-2,k] \n",
    "    f0 = features[:,k].copy()\n",
    "    df = pd.DataFrame(f0)\n",
    "    featureschange_df = df.pct_change()\n",
    "    featureschange_df.drop(0, axis = 0, inplace = True)\n",
    "    f0 = featureschange_df.T.iloc[:,-51:-2]\n",
    "\n",
    "\n",
    "\n",
    "    #Standardize\n",
    "\n",
    "    #f0 = sc.fit_transform(f0)  \n",
    "\n",
    "\n",
    "    #PCA on 5 components\n",
    "    f0 = pca.fit_transform(f0.T)\n",
    "\n",
    "\n",
    "    #Fit\n",
    "    reg1 = model.fit(f0, s0)\n",
    "    coef = reg1.coef_\n",
    "\n",
    "    s1 = (prices[-1][k]  - prices[-2][k]) / prices[-2][k] \n",
    "    predicted_f1 = coef.T * s1\n",
    "\n",
    "\n",
    "    ######################################################################\n",
    "\n",
    "\n",
    "    # F1 to S2\n",
    "\n",
    "    s1 = returns_df.iloc[-50:-1,k] \n",
    "    f0 = features[:,k].copy()\n",
    "    df = pd.DataFrame(f0)\n",
    "    featureschange_df = df.pct_change()\n",
    "    featureschange_df.drop(0, axis = 0, inplace = True)\n",
    "    f0 = featureschange_df.T.iloc[:,-51:-2]\n",
    "\n",
    "    f0 = pca.fit_transform(f0.T)\n",
    "    #f0_sqr = np.square(f0)\n",
    "    #f0 = np.concatenate([f0, f0_sqr], axis = 1)\n",
    "\n",
    "\n",
    "    reg2 = model.fit(f0, s1)\n",
    "    predicted_f1\n",
    "    #predicted_f1squared = np.square(predicted_f1)\n",
    "    #predicted_f1 = np.concatenate([predicted_f1, predicted_f1squared])\n",
    "\n",
    "    x = reg2.predict(predicted_f1.reshape(1,-1))\n",
    "    q.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = q.copy()\n",
    "z = np.stack(z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicted Prices\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_weights(n):\n",
    "    ''' Produces n random weights that sum to 1 '''\n",
    "    k = np.random.randn(n)\n",
    "    return k / sum(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_portfolio(returns, cov):\n",
    "    ''' \n",
    "    Returns the mean and standard deviation of returns for a random portfolio\n",
    "    '''\n",
    "\n",
    "    p = returns\n",
    "    w = np.asmatrix(rand_weights(returns.shape[0]))\n",
    "    C = cov\n",
    "    \n",
    "    mu = w * p\n",
    "    sigma = np.sqrt(w * C * w.T)\n",
    "    if m / (np.square(stds))\n",
    "    # This recursion reduces outliers to keep plots pretty\n",
    "    return mu, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a bunch of random portfolios and pick the highest sharpe ratio one\n",
    "#b/c this is hard jk I found a better way to do it so we are using that now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k = np.cov(returns_df.T)\n",
    "# n_portfolios = 10000\n",
    "# means, stds = np.column_stack([\n",
    "#     random_portfolio(z, k) \n",
    "#     for _ in range(n_portfolios)\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = means / (np.square(stds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p = z\n",
    "# C = k\n",
    "# portfolio = 0\n",
    "# count_change = 0\n",
    "# count_posmu = 0\n",
    "# for x in range(0,50000):\n",
    "#     w = np.asmatrix(rand_weights(returns_df.T.shape[0]))\n",
    "#     mu = w * p\n",
    "#     if mu > 0:\n",
    "#         count_posmu+=1\n",
    "#         sigma = np.sqrt(w * C * w.T)\n",
    "#         sharpe = (mu / np.square(sigma))\n",
    "#         if sharpe > portfolio:\n",
    "#             weights = w\n",
    "#             portfolio = sharpe\n",
    "#             count_change+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_posmu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.cov(features[-1].T)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = features[-1]\n",
    "sigma = np.cov(features[-1].T)\n",
    "\n",
    "cov_matrix = np.dot(np.dot(p, sigma), p.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = -opt.matrix(np.eye(n))   # negative n x n identity matrix\n",
    "h = opt.matrix(0.0, (n ,1))\n",
    "A = opt.matrix(1.0, (1, n))\n",
    "b = opt.matrix(1.0)\n",
    "    \n",
    "# Calculate efficient frontier weights using quadratic programming\n",
    "portfolios = [solvers.qp(mu*S, -pbar, G, h, A, b)['x'] \n",
    "                 for mu in mus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.cov(prices.T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "n = len(returns_df.T)\n",
    "S = matrix(np.cov(returns_df.T))\n",
    "pbar = matrix(z)\n",
    "G = matrix(0.0, (n,n))\n",
    "G[::n+1] = -1.0\n",
    "h = matrix(0.0, (n,1))\n",
    "A = matrix(1.0, (1,n))\n",
    "b = matrix(1.0)\n",
    "\n",
    "# Compute trade-off.\n",
    "N = 100\n",
    "mus = [ 10**(5.0*t/N-1.0) for t in range(N) ]\n",
    "#neg_mus = [ -10**(5.0*t/N-1.0) for t in range(N) ]\n",
    "#mus = neg_mus + mus\n",
    "portfolios = [ qp(mu*S, -pbar, G, h, A, b)['x'] for mu in mus ]\n",
    "returns = [ dot(pbar,x) for x in portfolios ]\n",
    "risks = [ sqrt(dot(x, S*x)) for x in portfolios ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sharpe = []\n",
    "for x, y in zip(returns, risks):\n",
    "    sharpe.append(x / np.square(y))\n",
    "import operator\n",
    "index, value = max(enumerate(sharpe), key=operator.itemgetter(1))\n",
    "print(np.array(portfolios[index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
