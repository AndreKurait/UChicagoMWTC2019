{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "import statsmodels.api as sm\n",
    "from sklearn import decomposition\n",
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from numpy import linalg\n",
    "import statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_object(file_name):\n",
    "    \"load the pickled object\"\n",
    "    with open(file_name, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "\n",
    "def view_data(data_path):\n",
    "    data = load_object(data_path)\n",
    "    prices = data['prices']\n",
    "    names = data['features']['names']\n",
    "    features = data['features']['values']\n",
    "    print(prices.shape)\n",
    "    print(names)\n",
    "    print(features.shape)\n",
    "    return prices, features, names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(757, 680)\n",
      "['labour cost', 'analyst projected total earnings', 'weighted average outstanding shares', 'R&D intensity index', 'relative strength index', 'total assets', 'net book value', 'analyst sentiment', 'market share', 'Aggregate Capital Cost']\n",
      "(756, 680, 10)\n"
     ]
    }
   ],
   "source": [
    "prices, features, names = view_data('C3_train.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prices1 = preprocessing.normalize(prices, norm = 'l2', axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forcast of S1 using only F0. (Noted as Eq. 2 in Latex Doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This is to test if massive inversions will cause machine error for inversions.\n",
    "'''inv = np.cov(prices1.T)\n",
    "inv2 = linalg.inv(np.cov(prices1.T))\n",
    "\n",
    "for x in range(0, 700):\n",
    "    inv2 = linalg.inv(inv2)\n",
    "    inv2 = linalg.inv(inv2)\n",
    "    \n",
    "x = np.matmul(inv, inv2)\n",
    "x\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.45377468 0.23047017 0.1186297  0.0925202  0.05994477 0.03027678\n",
      " 0.00989239 0.00426836]\n"
     ]
    }
   ],
   "source": [
    "s1 = prices[1:,1] \n",
    "f1 = features[:,1]\n",
    "f1squared = np.square(f1)\n",
    "f1 = np.concatenate([f1,f1squared], axis = 1)\n",
    "\n",
    "\n",
    "#Pull data\n",
    "X_train, X_test, y_train, y_test = train_test_split(f1, s1, test_size=0.2, random_state=0) \n",
    "\n",
    "#Standardize\n",
    "sc = StandardScaler()  \n",
    "X_train = sc.fit_transform(X_train)  \n",
    "X_test = sc.transform(X_test)  \n",
    "\n",
    "#PCA on 6 components\n",
    "pca = decomposition.PCA(n_components = 8)  \n",
    "X_train = pca.fit_transform(X_train)\n",
    "X_trainsquare = np.square(X_train)\n",
    "X_train = np.concatenate([X_train,X_trainsquare], axis = 1)\n",
    "X_test = pca.transform(X_test)\n",
    "X_testsquare = np.square(X_test)\n",
    "X_test = np.concatenate([X_test,X_testsquare], axis = 1)\n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "explained_variance = pca.explained_variance_ratio_  \n",
    "print(explained_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.13246328, -1.71406779,  0.70154737,  0.61320896, -2.18444642,\n",
       "       -0.5525321 , -1.20823111, -0.90292849,  0.06999308, -0.1729876 ,\n",
       "       -0.19142105, -0.08111851, -0.52223091, -0.35715616,  0.29594176,\n",
       "       -0.8906377 ])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg1 = model.fit(X_train, y_train)\n",
    "reg1.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time to create our L(ij) to use our prices at S1 to features at 1 (Equation 1 in Latex Doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.45377468 0.23047017 0.1186297  0.0925202  0.05994477 0.03027678\n",
      " 0.00989239 0.00426836]\n"
     ]
    }
   ],
   "source": [
    "s1 = prices[:756,1] \n",
    "f1 = features[:,1]\n",
    "\n",
    "#Pull data\n",
    "X_train, X_test, y_train, y_test = train_test_split(f1, s1, test_size=0.2, random_state=0) \n",
    "\n",
    "#Standardize\n",
    "sc = StandardScaler()  \n",
    "X_train = sc.fit_transform(X_train)  \n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "#PCA on 6 components\n",
    "pca = decomposition.PCA(n_components = 8)  \n",
    "X_train = pca.fit_transform(X_train)\n",
    "X_test = pca.transform(X_test)\n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "\n",
    "explained_variance = pca.explained_variance_ratio_  \n",
    "print(explained_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.19921414, -2.66588303,  0.65011621, -0.91317583,  1.75203833,\n",
       "       -0.96224694, -1.48356678, -0.75577469])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg = model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time to Update L(ij) based on real values of F2 (Equation 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Using some 'Real Value' that we will get, but we must prepare for it now. Take prices + WN(0,1) process value\n",
    "#the given F value\n",
    "realval = prices[-1] + np.random.normal()\n",
    "lij= -(reg.intercept_ - realval) / x "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time to Update the L(ij) based on the real values of S1 (Equation 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "We will also use a AR(1) model based on the previous value\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Price Forcasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#We will use equation 1 to estimate the features from the price at 1. \n",
    "#We will use equation 2 to estimate the prices at 2 from the featuress at 1\n",
    "#This will give us a price vector that we can estimate on"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
